{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ydkEQ6gD-3S_",
        "tBQS3gLX_ZDO",
        "tGgV3oSM7DD3",
        "WOZOVg1uhplL",
        "GP1SqdpsNgbE",
        "g_410QQ7hwgQ",
        "dRgiGiqiAGfs",
        "TNNnnuLhwx4O",
        "On8szvRl8S5j",
        "15coZX8g4ild",
        "hr2huffGSbp7",
        "pR_CGGX3-N_F",
        "epR0U5xvh1R0",
        "TdSQYwBNN9Er",
        "rOWHtD2NF5WE",
        "dbxK1Jy1Yx7e"
      ],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hridoy1750/Developer-Portfolio/blob/main/Fake_Job_Posting_Detection_Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djKiVw_VcCGP"
      },
      "source": [
        "# Predicting Real & Fake Job Postings\n",
        "Dataset obtained from Kaggle: https://www.kaggle.com/shivamb/real-or-fake-fake-jobposting-prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydkEQ6gD-3S_"
      },
      "source": [
        "## Importing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlRVrRWnzDk_",
        "outputId": "862f44cd-31db-4f86-9da2-3b03e50b6bfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.python.ops.math_ops import reduce_prod\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.layers import Embedding"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-153684007.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZhPplkyzGlH",
        "collapsed": true
      },
      "source": [
        "jobs_file = tf.keras.utils.get_file(\n",
        "    fname=\"fraudulent_jobs.csv\",\n",
        "    origin=\"https://uofi.box.com/shared/static/sfw0eqvj7q49vmexpztqke7xzhspevnb.csv\"\n",
        ")\n",
        "\n",
        "df=pd.read_csv(jobs_file)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4bkKjWq_Nig",
        "collapsed": true
      },
      "source": [
        "#drop jobs_id column and update 'work_remote' to 'work_remote's\n",
        "df.drop(columns=['job_id'], inplace=True)\n",
        "df.rename(columns={'telecommuting': 'work_remote'}, inplace=True)\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBQS3gLX_ZDO"
      },
      "source": [
        "## Exploring the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ftz25P_v0Q3"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLeovrdp5vAR"
      },
      "source": [
        "df.nunique() #get # of unique values in dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk82ND44VvRS"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaVSN_KAv0Gw"
      },
      "source": [
        "df['fraudulent'].value_counts()\n",
        "#data is very imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF_uo_M1zvsx"
      },
      "source": [
        "* Text columns are title, location, department, company profile, description, requirements, and benefits, industry, function.\n",
        "* Categorical columns are employment type, required experience, and required education\n",
        "* Numeric variables are salary (low/high), work_remote, has company logo, and has questions\n",
        "* Column to Predict is fraudulent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgdq_0mogOyi"
      },
      "source": [
        "#separate columns based on their type\n",
        "text_cols = ['title', 'location', 'department', 'company_profile', 'description', 'requirements', 'benefits']\n",
        "categorical_cols = ['employment_type', 'required_experience', 'required_education', 'industry', 'function']\n",
        "numeric_cols = ['work_remote', 'has_company_logo', 'has_questions', 'salary_low', 'salary_high']\n",
        "col_to_predict = ['fraudulent']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZyCoFGo-6gY"
      },
      "source": [
        "## Cleaning Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGgV3oSM7DD3"
      },
      "source": [
        "### Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CZna4WQaSM4"
      },
      "source": [
        "def move_column(df, col):\n",
        "  df['Temp_Col'] = df[col]\n",
        "  df.drop(columns=[col], inplace=True)\n",
        "  df.rename(columns={'Temp_Col': col}, inplace=True)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnlmbxGU7DAY"
      },
      "source": [
        "df['employment_type'].fillna('No Data', inplace=True)\n",
        "df['required_experience'].fillna('No Data', inplace=True)\n",
        "df['required_education'].fillna('No Data', inplace=True)\n",
        "df['industry'].fillna('No Data', inplace=True)\n",
        "df['function'].fillna('No Data', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqk3jxR21Ca6"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EpuB23bBiZ0"
      },
      "source": [
        "df.notna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dv7T0iU3Fsi"
      },
      "source": [
        "df[df['description'].isna()] #only 1 row with an empty description -- the post only has title and location and it is a fraud post --> keeping it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViknTDR21ycc"
      },
      "source": [
        "#rows that are entirely null in the text columns\n",
        "null_text = df[(df['location'].isna()) & (df['department'].isna()) & (df['company_profile'].isna()) & (df['requirements'].isna()) & (df['benefits'].isna())]\n",
        "# 'title', 'location', 'department', 'company_profile', 'description', 'requirements', 'benefits'\n",
        "print(f\"There are {null_text.shape[0]} rows where all text columns aside from 'title' and 'description' are empty. Of these rows, {null_text['fraudulent'].sum()} are fraudulent posts.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4EcIxqzO7Yr"
      },
      "source": [
        "for col in text_cols:\n",
        "  df[col].fillna(value=\" \", inplace=True)\n",
        "  print(f\"Max length of '{col}': {df[col].map(len).max()}\") #get maximum number of characters in each column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o4sIFfp4qW4"
      },
      "source": [
        "#combine all text into 1 column - full_text\n",
        "df['full_text'] = df['title'] + \" \" + df['location'] + \" \" + df['department']  + \" \" + df['company_profile']  + \" \" + df['description']  + \" \" + df['requirements']  + \" \"  + df['benefits']\n",
        "df['full_text'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KkiK8_QR4O8"
      },
      "source": [
        "print(f\"Min length of 'full text': {df['full_text'].map(len).min()}\")\n",
        "print(f\"Max length of 'full text': {df['full_text'].map(len).max()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9eLbK-OhCPs"
      },
      "source": [
        "def update_text(new_df):\n",
        "  new_df['full_text'] = new_df['full_text'].str.replace(pat='US', repl='USA', case=True)\n",
        "  new_df['location'] = new_df['location'].str.replace(pat='US', repl='USA', case=True)\n",
        "  return new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMUN_EulF9LF"
      },
      "source": [
        "# update US with USA so it does not get mixed up with the word 'us'\n",
        "df = update_text(df)\n",
        "\n",
        "#replace urls, email, phone numbers (contact details) ?? maybe presence/absence could indicate a fake post?\n",
        "# df['full_text'] = df['full_text'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', ' ', regex=True).replace(r'#PHONE\\S+', '', regex=True).replace(r'#EMAIL\\S+', '', regex=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zRS1yc_Qugy"
      },
      "source": [
        "#separate 2 separate words that have been put together (i.e. PinterestLoves -> Pinterest Loves)\n",
        "def space_words(all_text):\n",
        "  import re\n",
        "  # print('start', all_text)\n",
        "  result = re.sub('(?<=[A-Za-z])(?=[A-Z][a-z])', '~', all_text)\n",
        "  result = re.split('~', result)\n",
        "  result = ' '.join(result)\n",
        "  all_text = result\n",
        "  # print('end',all_text)\n",
        "  return all_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP8iDFhfRFSE"
      },
      "source": [
        "texts = ['title', 'location', 'department', 'company_profile', 'description',\n",
        "       'requirements', 'benefits', 'employment_type', 'required_experience',\n",
        "       'required_education', 'industry', 'function', 'full_text']\n",
        "\n",
        "for col in texts:\n",
        "  df[col] = df[col].apply(space_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27UfnlSfw4IQ"
      },
      "source": [
        "df.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyCbcf_P0ESl"
      },
      "source": [
        "def get_unique_words(new_df, col):\n",
        "  unique_words = []\n",
        "  idx = list(new_df.index.values)\n",
        "  for i in idx:\n",
        "    x = list(new_df[col][i].split())\n",
        "    unique_words += x\n",
        "  return unique_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCTCsDCO13n_"
      },
      "source": [
        "for col in texts:\n",
        "  print(f'Number of total words in {col}: {len(get_unique_words(df, col))}')\n",
        "  print(f'Number of unique words in {col}: {len(set(get_unique_words(df, col)))}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOZOVg1uhplL"
      },
      "source": [
        "### Salary Range Column\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuC46jOt9u_A"
      },
      "source": [
        "salary_df = df['salary_range'].str.split(pat='-', n=-1, expand=True) #separate salary range into two columns\n",
        "salary_df[0].unique()[90:100] #months (strings) are included as salaries due to csv interpretation of the salary range"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6z5wc3f51hV"
      },
      "source": [
        "#convert the months in the salary columns into integers\n",
        "months_to_int = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n",
        "\n",
        "for i in range(salary_df[0].shape[0]):\n",
        "  if salary_df[0][i] in months_to_int.keys():\n",
        "    salary_df[0][i] = months_to_int[salary_df[0][i]]\n",
        "\n",
        "for i in range(salary_df[1].shape[0]):\n",
        "  if salary_df[1][i] in months_to_int.keys():\n",
        "    salary_df[1][i] = months_to_int[salary_df[1][i]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F9HLudq4ZBC"
      },
      "source": [
        "#verify conversion of string months to integers\n",
        "salary_df[0].unique()[90:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6QZcgSmAiwA"
      },
      "source": [
        "salary_df.fillna(value=-1, inplace=True)\n",
        "salary_df[0] = np.array(salary_df[0], dtype='int64')\n",
        "salary_df[1] = np.array(salary_df[1], dtype='int64')\n",
        "\n",
        "#move the min/max salary columns to the main dataframe\n",
        "df['salary_low'] = salary_df[0]\n",
        "df['salary_high'] = salary_df[1]\n",
        "df.drop(columns=['salary_range'], inplace=True)\n",
        "\n",
        "df['salary_low'].replace(-1, np.nan, inplace=True)\n",
        "df['salary_high'].replace(-1, np.nan, inplace=True)\n",
        "\n",
        "df['salary_low'] = df['salary_low'].astype('Int64')\n",
        "df['salary_high'] = df['salary_high'].astype('Int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTTJG_cyx8I9"
      },
      "source": [
        "df[df['salary_low'].notnull() & df['salary_high'].notnull()][:3] #some examples showing that the salary was converted back to an int value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWv4sc2nm9G0"
      },
      "source": [
        "#### Replace Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0kRTa741ao5"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#split df into train/val/test so I can use the mean of salary_low & salary_high from the training data\n",
        "print('dataframe shape:',df.shape)\n",
        "\n",
        "train, test_x = train_test_split(df, train_size=0.8, random_state=1, shuffle=True, stratify=df.fraudulent.values)\n",
        "print(\"train shape:\",train.shape)\n",
        "print('test shape:',test_x.shape)\n",
        "\n",
        "train_x, val_x = train_test_split(train, train_size = 0.8, random_state=1, shuffle = True, stratify=train.fraudulent.values)\n",
        "print(\"train_x shape:\",train_x.shape)\n",
        "print('val shape:',val_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxBHf0k_3IiM"
      },
      "source": [
        "train_x['employment_type'].isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4z84E5XiLib"
      },
      "source": [
        "# Assuming 'numeric_cols' is a list containing the names of the numeric columns:\n",
        "train_x[numeric_cols].groupby(train_x['employment_type']).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA3YzYUsiW9z"
      },
      "source": [
        "salary_means = {}\n",
        "employment_types = list(train_x['employment_type'].unique())\n",
        "employment_types.sort()\n",
        "employment_types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijoso8RJihSZ"
      },
      "source": [
        "low_means = list(train_x.groupby('employment_type')['salary_low'].mean())\n",
        "high_means = list(train_x.groupby('employment_type')['salary_high'].mean())\n",
        "salary_means = {}\n",
        "for i in range(len(employment_types)):\n",
        "  salary_means[employment_types[i]] = [int(low_means[i]), int(high_means[i])]\n",
        "\n",
        "print(\"List of Means (from train_x dataset):\", salary_means)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e5Xmokm1amd"
      },
      "source": [
        "for et in employment_types:\n",
        "    temp = df[df['employment_type'] == et] #separate the group\n",
        "    temp['salary_low'].fillna(salary_means[et][0], inplace=True)\n",
        "    temp['salary_high'].fillna(salary_means[et][1], inplace=True)\n",
        "    df.update(temp, overwrite=False)\n",
        "\n",
        "df['salary_low'] = df['salary_low'].astype('int64')\n",
        "df['salary_high'] = df['salary_high'].astype('int64')\n",
        "print(df.shape)\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfs72YqOo0aY"
      },
      "source": [
        "df.info() #confirms that the salary columns are int64 and not float64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP1SqdpsNgbE"
      },
      "source": [
        "### Additional Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBEdyqq-Ezyf"
      },
      "source": [
        "df_preproc_done = df.copy()\n",
        "df_preproc_done.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qURY5a_CAdOK"
      },
      "source": [
        "# combine all text columns into the title column\n",
        "# drop all text columns except full_text\n",
        "# rename 'title' column to 'full_text'\n",
        "df['title'] = df['full_text']\n",
        "df.drop(columns=['location', 'department', 'company_profile', 'description', 'requirements', 'benefits', 'full_text'], inplace=True)\n",
        "df.rename(columns={'title': 'full_text'}, inplace=True)\n",
        "\n",
        "#put fraud column at the end of the dataframe\n",
        "df = move_column(df, 'fraudulent')\n",
        "df.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHRHRM7q0B9Q"
      },
      "source": [
        "Completed:\n",
        "* replaced NA values in categorical columns with 'No Data'\n",
        "* created 'full_text' column with all textual columns combined\n",
        "* split the 'salary_range' column into 2 for the range --> salary_low, salary_high\n",
        "* replace 'months' in salary columns with integers\n",
        "* replace NA values in salary columns with -1, create new columns in main dataframe for salary (low/high), replace -1 with NaN and cast to type Integer64\n",
        "* move full_text to the front of the dataframe, move fraudulent to the end of the dataframe; and rename both columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_410QQ7hwgQ"
      },
      "source": [
        "## Create real and fraud datasets (visualization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M3rejxXESm4"
      },
      "source": [
        "df['fraudulent'].value_counts() #data is very imbalanced towards real posts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW9vG5PWkxmy"
      },
      "source": [
        "real_df = df[df['fraudulent'] == 0]\n",
        "fraud_df = df[df['fraudulent'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m42WBuKq3LrI"
      },
      "source": [
        "real_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUudSb4R3frf"
      },
      "source": [
        "fraud_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqUFG1ur4q1-"
      },
      "source": [
        "print(\"Min and Max Lengths of 'full_text' in train, val, test ----\")\n",
        "print(f\"train - min: {train_x['full_text'].map(len).min()}, max: {train_x['full_text'].map(len).max()}\")\n",
        "print(f\"val - min: {val_x['full_text'].map(len).min()}, max: {val_x['full_text'].map(len).max()}\")\n",
        "print(f\"test - min: {test_x['full_text'].map(len).min()}, max: {test_x['full_text'].map(len).max()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRgiGiqiAGfs"
      },
      "source": [
        "## Visualizing the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EklzlusyudF"
      },
      "source": [
        "import collections\n",
        "from collections import Counter\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfexmcWkEfDx"
      },
      "source": [
        "sns.countplot(x='fraudulent', data=df, palette='hls')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2av85h7HxGmV"
      },
      "source": [
        "df['employment_type'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jtCS7lOGuyX"
      },
      "source": [
        "table=pd.crosstab(df.employment_type, df.fraudulent)\n",
        "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
        "plt.title('Stacked Bar Chart of Employment Type vs Fraud Posts')\n",
        "plt.xlabel('Employment Type')\n",
        "plt.ylabel('Fraud Posts')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF-B8Ds4xUMU"
      },
      "source": [
        "df['required_experience'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEUgflHTFcIq"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "table=pd.crosstab(df.required_experience, df.fraudulent)\n",
        "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
        "plt.title('Fraud Posts Based on Required Experience')\n",
        "plt.xlabel('Required Experience')\n",
        "plt.ylabel('Fraud Posts')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h0ENu09xaiZ"
      },
      "source": [
        "df['required_education'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4k8_4LYGCZq"
      },
      "source": [
        "table=pd.crosstab(df.required_education, df.fraudulent)\n",
        "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
        "plt.title('Stacked Bar Chart of Required Education vs Fraud Posts')\n",
        "plt.xlabel('Required Education')\n",
        "plt.ylabel('Fraud Posts')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrehZ1Kdxgao"
      },
      "source": [
        "df[(df['required_education'] == 'Some High School Coursework') & (df['fraudulent'] == 1)].shape[0] #20 fraud posts out of 27 total posts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up5IYbT-4sWy"
      },
      "source": [
        "#top 20 most common industries - ALL POSTS\n",
        "industry=list(df['industry'])\n",
        "counts = Counter(industry).most_common(20)\n",
        "counts_df = pd.DataFrame(counts)\n",
        "counts_df.columns=['Industry', 'Number of Posts']\n",
        "fig, ax = plt.subplots(figsize=(12,12))\n",
        "ax = sns.barplot(y='Industry', x='Number of Posts', ax=ax, data=counts_df)\n",
        "plt.title(\"Top 20 Most Common Industries Listed in All Job Postings\")\n",
        "plt.show()\n",
        "#top 5: no data, information technology&services, computer software, internet, education management"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-_fMKKmVKe8"
      },
      "source": [
        "#top 20 most common industries - FRAUD POSTS\n",
        "industry=list(fraud_df['industry'])\n",
        "counts = Counter(industry).most_common(20)\n",
        "counts_df = pd.DataFrame(counts)\n",
        "counts_df.columns=['Industry', 'Number of Posts']\n",
        "fig, ax = plt.subplots(figsize=(12,12))\n",
        "ax = sns.barplot(y='Industry', x='Number of Posts', ax=ax, data=counts_df)\n",
        "plt.title(\"Top 20 Most Common Industries Listed in Fake Posts\")\n",
        "plt.show()\n",
        "#top 5: no data, oil&energy, accounting, hospital&health care, marketing&advertising"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoWzniIDrHJx"
      },
      "source": [
        "#top 20 most common industries - REAL POSTS\n",
        "industry=list(real_df['industry'])\n",
        "counts = Counter(industry).most_common(20)\n",
        "counts_df = pd.DataFrame(counts)\n",
        "counts_df.columns=['Industry', 'Number of Posts']\n",
        "fig, ax = plt.subplots(figsize=(12,12))\n",
        "ax = sns.barplot(y='Industry', x='Number of Posts', ax=ax, data=counts_df)\n",
        "plt.title(\"Top 20 Most Common Industries Listed in Real Posts\")\n",
        "plt.show()\n",
        "#top 5: no data, information technology&services, computer software, internet, education management"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5BHrMGF5bY5"
      },
      "source": [
        "#top 20 most common functions in ALL JOB POSTINGS\n",
        "function=list(df['function'])\n",
        "counts = Counter(function).most_common(20)\n",
        "counts_df = pd.DataFrame(counts)\n",
        "counts_df.columns=['Function', 'Number of Posts']\n",
        "fig, ax = plt.subplots(figsize=(12,12))\n",
        "ax = sns.barplot(y='Function', x='Number of Posts', ax=ax, data=counts_df)\n",
        "plt.title('Top 20 Most Common Functions among All Job Postings')\n",
        "plt.show()\n",
        "#top 5 = no data, administrative, engineering, customer service, sales"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J15Exb-_XndV"
      },
      "source": [
        "#top 20 most common functions in fraud posts\n",
        "function=list(fraud_df['function'])\n",
        "counts = Counter(function).most_common(20)\n",
        "counts_df = pd.DataFrame(counts)\n",
        "counts_df.columns=['Function', 'Number of Posts']\n",
        "fig, ax = plt.subplots(figsize=(12,12))\n",
        "ax = sns.barplot(y='Function', x='Number of Posts', ax=ax, data=counts_df)\n",
        "plt.title('Top 20 Most Common Functions in Fraud Posts')\n",
        "plt.show()\n",
        "#top 5 = no data, administrative, engineering, customer service, sales"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soy2M7x3qfWB"
      },
      "source": [
        "#top 20 most common functions in real posts\n",
        "function=list(real_df['function'])\n",
        "counts = Counter(function).most_common(20)\n",
        "counts_df = pd.DataFrame(counts)\n",
        "counts_df.columns=['Function', 'Number of Posts']\n",
        "fig, ax = plt.subplots(figsize=(12,12))\n",
        "ax = sns.barplot(y='Function', x='Number of Posts', ax=ax, data=counts_df)\n",
        "plt.title('Top 20 Most Common Functions in Real Posts')\n",
        "plt.show()\n",
        "#top 5 = no data, information technology, sales, engineering, customer service"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6No-pnnkmCXG"
      },
      "source": [
        "table=pd.crosstab(df.work_remote, df.fraudulent)\n",
        "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar')\n",
        "plt.title('Stacked Bar Chart of \"Work_Remote\"')\n",
        "plt.xlabel('Remote Work')\n",
        "plt.ylabel('Fraud Posts')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6FIJxrwpS_b"
      },
      "source": [
        "table=pd.crosstab(df.has_company_logo, df.fraudulent)\n",
        "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar')\n",
        "plt.title('Stacked Bar Chart of \"Has Company Logo\"')\n",
        "plt.xlabel('Has Company Logo')\n",
        "plt.ylabel('Fraud Posts')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swKRm_lUpk2R"
      },
      "source": [
        "table=pd.crosstab(df.has_questions, df.fraudulent)\n",
        "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar')\n",
        "plt.title('Stacked Bar Chart of \"Has Questions\"')\n",
        "plt.xlabel('Has Questions')\n",
        "plt.ylabel('Fraud Posts')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNNnnuLhwx4O"
      },
      "source": [
        "## One-Hot Encode Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWO8BSRfdrcE"
      },
      "source": [
        "print(\"Shape before one-hot-encoding:\", df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrgTTgxvwvx3"
      },
      "source": [
        "df = pd.get_dummies(df, columns=['employment_type', 'required_experience', 'required_education', 'industry', 'function']) #categorical_cols = ['employment_type', 'required_experience', 'required_education', 'industry', 'function']\n",
        "print('Shape after one-hot-encoding:',df.shape) #(17880, 205)\n",
        "df.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On8szvRl8S5j"
      },
      "source": [
        "## Split Data into Train, Val, and Test Sets\n",
        "\n",
        "The data had already been split to get the salary mean for the training set, but now I will re-split it again (random_state = 1 will ensure that results were same as before) now that there are no NA values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQkD_f9_i9af"
      },
      "source": [
        "print('dataframe shape:',df.shape)\n",
        "train, test_x = train_test_split(df, train_size=0.8, random_state=1, shuffle=True, stratify=df.fraudulent.values)\n",
        "# print(\"train shape:\",train.shape)\n",
        "\n",
        "train_x, val_x= train_test_split(train, train_size = 0.8, random_state=1, shuffle = True, stratify=train.fraudulent.values)\n",
        "print(\"train_x shape:\",train_x.shape)\n",
        "print('val shape:',val_x.shape)\n",
        "print('test shape:',test_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAFNbly4_ML1"
      },
      "source": [
        "train_x.nunique() #can't see all unique values so going to break it down into 5 sets of 41 columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xCw3WdBg7hC"
      },
      "source": [
        "train_x.nunique()[:41]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUmFomug3zn"
      },
      "source": [
        "train_x.nunique()[41:82]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRkfsNSSg3B8"
      },
      "source": [
        "train_x.nunique()[82:123]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsLQNMMpg259"
      },
      "source": [
        "train_x.nunique()[123:164]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyqFf7pwg2vp"
      },
      "source": [
        "train_x.nunique()[164:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJo_VDJTxMqE"
      },
      "source": [
        "There are 5 industries where the training data only has 1 option:\n",
        "* industry_Libraries\n",
        "* industry_Military\n",
        "* industry_Package/Freight Delivery\n",
        "* industry_Shipbuilding\n",
        "* industry_Wine and Spirits\n",
        "\n",
        "This means that the models are only learning 1 answer from the training data  for these five columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvjjKhISwTJj"
      },
      "source": [
        "print('industry_Libraries')\n",
        "print('Train:\\n',train_x['industry_Libraries'].value_counts())\n",
        "print('Val:\\n', val_x['industry_Libraries'].value_counts())\n",
        "print('Test:\\n', test_x['industry_Libraries'].value_counts())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRrN_XS3yC-S"
      },
      "source": [
        "print('industry_Military')\n",
        "print('Train:\\n',train_x['industry_Military'].value_counts())\n",
        "print('Val:\\n', val_x['industry_Military'].value_counts())\n",
        "print('Test:\\n', test_x['industry_Military'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izOGj3-myCwb"
      },
      "source": [
        "print('industry_Package/Freight Delivery')\n",
        "print('Train:\\n',train_x['industry_Package/Freight Delivery'].value_counts())\n",
        "print('Val:\\n', val_x['industry_Package/Freight Delivery'].value_counts())\n",
        "print('Test:\\n', test_x['industry_Package/Freight Delivery'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6I83JrIyCi8"
      },
      "source": [
        "print('industry_Shipbuilding')\n",
        "print('Train:\\n',train_x['industry_Shipbuilding'].value_counts())\n",
        "print('Val:\\n', val_x['industry_Shipbuilding'].value_counts())\n",
        "print('Test:\\n', test_x['industry_Shipbuilding'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rJLLz7cyB9P"
      },
      "source": [
        "print('industry_Wine and Spirits')\n",
        "print('Train:\\n',train_x['industry_Wine and Spirits'].value_counts())\n",
        "print('Val:\\n', val_x['industry_Wine and Spirits'].value_counts())\n",
        "print('Test:\\n', test_x['industry_Wine and Spirits'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dHjRA_Vrfzn"
      },
      "source": [
        "print(val_x.nunique()[:41])\n",
        "print(val_x.nunique()[41:82])\n",
        "print(val_x.nunique()[82:123])\n",
        "print(val_x.nunique()[123:164])\n",
        "print(val_x.nunique()[164:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnml-aQTrxnt"
      },
      "source": [
        "print(test_x.nunique()[:41])\n",
        "print(test_x.nunique()[41:82])\n",
        "print(test_x.nunique()[82:123])\n",
        "print(test_x.nunique()[123:164])\n",
        "print(test_x.nunique()[164:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTpx2Iw2_XbD"
      },
      "source": [
        "train_x['fraudulent'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDSM4Wlv_QbP"
      },
      "source": [
        "sns.countplot(x='fraudulent', data=train_x, palette='hls')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4tWWyOE0yjl"
      },
      "source": [
        "## Reducing the Training Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWr4ByHB2IsT"
      },
      "source": [
        "train_x['fraudulent'].value_counts() #I will keep all 554 fraudulent posts and instead just subsample the real posts using the sample(frac=0.5, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak3cWIuE2Ipn"
      },
      "source": [
        "def undersample_data(new_df, col_name, val, fraction, seed):\n",
        "  undersampled = new_df[new_df[col_name] == val]\n",
        "  undersampled=undersampled.sample(frac=fraction, random_state=seed)\n",
        "  # print(undersampled.shape)\n",
        "  # print(undersampled['fraudulent'].value_counts()) #making sure only real data has been subsampled\n",
        "  return undersampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljz5cc9a2Im5"
      },
      "source": [
        "fake_train = train_x[train_x['fraudulent'] == 1]\n",
        "fake_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "under_train_x = pd.concat([undersampled, fake_train], ignore_index=True) # use pd.concat to combine DataFrames"
      ],
      "metadata": {
        "id": "5IYk6HWYrGgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N10rTrcL4-z4"
      },
      "source": [
        "undersampled = undersample_data(train_x, 'fraudulent', 0, 0.5, 1)\n",
        "under_train_x = pd.concat([undersampled, fake_train], ignore_index=True) # use pd.concat to combine DataFrames\n",
        "under_train_x['fraudulent'].value_counts()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks6Glp2GZrwb"
      },
      "source": [
        "under_train_x_labels = under_train_x['fraudulent']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSr1WCA299Lt"
      },
      "source": [
        "sns.countplot(x='fraudulent', data=under_train_x, palette='hls') #approximately a 90/10 ratio\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43QCJ6J9mH4_"
      },
      "source": [
        "## Base Model - RNN with Embedding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJDiN-fk2vLq"
      },
      "source": [
        "### Numeric + Text Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "940kEIBT-DgN"
      },
      "source": [
        "#### Building the Model with Numeric and Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2SSL2lN9roQ"
      },
      "source": [
        "#TextVectorization layer:\n",
        "#turns raw strings into an encoded representation that can be read by an Embedding layer or Dense layer\n",
        "#TextVectorization(max_tokens, standardize, split, ngrams, output_mode, output_sequence_length, pad_to_max_tokens)\n",
        "\n",
        "def vectorize(text):\n",
        "  vectorize_layer = TextVectorization(\n",
        "      output_mode='int',\n",
        "      max_tokens=128)\n",
        "      #output_sequence_length=512 gave 125,764 total vocabulary size\n",
        "\n",
        "  vectorize_layer.adapt(np.asarray(text)) #builds vocabulary\n",
        "  return vectorize_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOn6K_YsyTQt"
      },
      "source": [
        "text=train_x['full_text'].values\n",
        "text[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlUv30rS4oUY"
      },
      "source": [
        "vocab = vectorize(text)\n",
        "vocab = list(vocab.get_vocabulary())\n",
        "print(f\"Total Length of vocab: {len(vocab)}\\nTop 20 words in vocab: {vocab[:20]}\\nLast 20 words in vocab: {vocab[-20:]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIzQZNgd4S7n"
      },
      "source": [
        "#Embedding Layer:\n",
        "#Turns positive integers (indexes) into dense vectors of fixed size\n",
        "# This layer can only be used as the first layer in a model\n",
        "\n",
        "# tf.keras.layers.Embedding(\n",
        "#     input_dim,\n",
        "#     output_dim,\n",
        "#     embeddings_initializer=\"uniform\",\n",
        "#     embeddings_regularizer=None,\n",
        "#     activity_regularizer=None,\n",
        "#     embeddings_constraint=None,\n",
        "#     mask_zero=False,\n",
        "#     input_length=None,\n",
        "#     **kwargs\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D58n-y25PfEr"
      },
      "source": [
        "train_text = train_x['full_text'].to_numpy()\n",
        "val_text=val_x['full_text'].to_numpy()\n",
        "test_text = test_x['full_text'].to_numpy()\n",
        "undersampled_text = under_train_x['full_text'].to_numpy()\n",
        "\n",
        "print(train_text.shape, val_text.shape, test_text.shape, undersampled_text.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db-ALkIL8BTg"
      },
      "source": [
        "train_numeric = train_x.drop(['full_text', 'fraudulent'], axis=1).to_numpy()\n",
        "val_numeric=val_x.drop(['full_text', 'fraudulent'], axis=1).to_numpy()\n",
        "test_numeric = test_x.drop(['full_text', 'fraudulent'], axis=1).to_numpy()\n",
        "undersampled_numeric = under_train_x.drop(['full_text', 'fraudulent'], axis=1).to_numpy()\n",
        "\n",
        "print(train_numeric.shape, val_numeric.shape, test_numeric.shape, undersampled_numeric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C2xEm4TP8GH"
      },
      "source": [
        "train_labels = train_x['fraudulent'].to_numpy()\n",
        "val_labels = val_x['fraudulent'].to_numpy()\n",
        "test_labels = test_x['fraudulent'].to_numpy()\n",
        "undersampled_labels = under_train_x['fraudulent'].to_numpy()\n",
        "\n",
        "print(train_labels.shape, val_labels.shape, test_labels.shape, undersampled_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6O5s-yDQMVi"
      },
      "source": [
        "print('Text:',train_text[0])\n",
        "print('Numeric Variables:\\n',train_numeric[0])\n",
        "print('Label:',train_labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFzTnyOsZ9gw"
      },
      "source": [
        "metrics_list = [\n",
        "        keras.metrics.FalsePositives(name='fp'),\n",
        "        keras.metrics.FalseNegatives(name='fn'),\n",
        "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "        keras.metrics.AUC(name='auc')\n",
        "  ]\n",
        "\n",
        "def build_rnn_model():\n",
        "  text_inputs=tf.keras.Input(shape=(1,), dtype=tf.string, name='text_inputs')\n",
        "  text_outputs = vectorize(text_inputs) #text_outputs.dtype = 'tf.int64'\n",
        "  i = layers.Embedding(len(vectorize.get_vocabulary()), 128, input_length=128, mask_zero=True)(text_outputs) #changed from input_length=512\n",
        "  i=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(i)\n",
        "\n",
        "  numeric_inputs = tf.keras.Input(shape=(203,), name='numeric_inputs')\n",
        "  j=layers.Dense(128, activation='relu', name='dense_j1',\n",
        "                kernel_initializer=tf.keras.initializers.he_normal(seed=1))(numeric_inputs)\n",
        "  j=layers.Dropout(0.1)(j)\n",
        "  j=layers.Dense(128, activation='relu', name='dense_j2',\n",
        "                kernel_initializer=tf.keras.initializers.he_normal(seed=1))(j) #j.dtype='float32'\n",
        "\n",
        "  inputs = keras.layers.concatenate([i, j])\n",
        "  x = layers.Dense(32, activation='relu', name='layer1')(inputs)\n",
        "  # x=layers.Dropout(0.2, name='dropout1')(x)\n",
        "  # x = layers.Dense(32, activation='relu', name='layer2')(x)\n",
        "  # x=layers.Dropout(0.2, name='dropout2')(x)\n",
        "  output=layers.Dense(1, activation='sigmoid', name='final_output')(x)\n",
        "\n",
        "  rnn_model = Model(inputs=[text_inputs, numeric_inputs], outputs=[output])\n",
        "  return rnn_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm1Y52qPa0L7"
      },
      "source": [
        "\n",
        "model_rnn = build_rnn_model()\n",
        "print(model_rnn.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qblHL_U0Z9WU"
      },
      "source": [
        "keras.utils.plot_model(model_rnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15coZX8g4ild"
      },
      "source": [
        "#### Training the model with text + numeric variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZMS0F906fb9"
      },
      "source": [
        "from statistics import mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akag8oL4a4Jd"
      },
      "source": [
        "model_rnn.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "                    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                    metrics=metrics_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all numeric columns in train_numeric and val_numeric to float32\n",
        "train_numeric = train_x.drop(['full_text', 'fraudulent'], axis=1).astype(np.float32).to_numpy()\n",
        "val_numeric = val_x.drop(['full_text', 'fraudulent'], axis=1).astype(np.float32).to_numpy()\n",
        "# In your model.fit call:\n",
        "history = model_rnn.fit([train_text, train_numeric], train_labels, epochs=5, validation_data=([val_text, val_numeric], val_labels))"
      ],
      "metadata": {
        "id": "5BPX3nn0ubWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0OLVH4HdfE5"
      },
      "source": [
        "history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffu08ThoOgXd"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, label='Training loss')\n",
        "plt.plot(epochs, val_loss, label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cpBcH4IOgUe"
      },
      "source": [
        "auc = history.history['auc']\n",
        "val_auc = history.history['val_auc']\n",
        "epochs = range(1, len(auc) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, auc, label='Training AUC')\n",
        "plt.plot(epochs, val_auc, label='Validation AUC')\n",
        "plt.title('Training and validation AUC')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlGE0UQ_nffl"
      },
      "source": [
        "fp = history.history['fp']\n",
        "val_fp = history.history['val_fp']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, fp, label='Training False Positives')\n",
        "plt.plot(epochs, val_fp, label='Validation False Positives')\n",
        "plt.title('Training and validation False Positives')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J8hxWwcnfVy"
      },
      "source": [
        "fn = history.history['fn']\n",
        "val_fn = history.history['val_fn']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, label='Training False Negatives')\n",
        "plt.plot(epochs, val_loss, label='Validation False Negatives')\n",
        "plt.title('Training and validation False Negatives')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2JveCJiaoRc"
      },
      "source": [
        "#### Comparing these results with training the model on undersampled training data (validation data remains the same)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qePFjJUfa9aF"
      },
      "source": [
        "model_rnn.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "                    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                    metrics=metrics_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all numeric columns in train_numeric and val_numeric to float32\n",
        "undersampled_numeric = under_train_x.drop(['full_text', 'fraudulent'], axis=1).astype(np.float32).to_numpy()\n",
        "val_numeric = val_x.drop(['full_text', 'fraudulent'], axis=1).astype(np.float32).to_numpy()\n",
        "# In your model.fit call:\n",
        "history = model_rnn.fit([undersampled_text, undersampled_numeric], undersampled_labels, epochs=5, validation_data=([val_text, val_numeric], val_labels))"
      ],
      "metadata": {
        "id": "ahclh9F6AlC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA6johSBp56i"
      },
      "source": [
        "history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all numeric columns in train_numeric and val_numeric to float32\n",
        "undersampled_numeric = under_train_x.drop(['full_text', 'fraudulent'], axis=1).astype(np.float32).to_numpy()\n",
        "val_numeric = val_x.drop(['full_text', 'fraudulent'], axis=1).astype(np.float32).to_numpy()\n",
        "#undersampled_text and val_text should already be strings, but confirming here\n",
        "undersampled_text = under_train_x['full_text'].astype(str).to_numpy()\n",
        "val_text = val_x['full_text'].astype(str).to_numpy()\n",
        "\n",
        "# In your model.fit call:\n",
        "history = model_rnn.fit([undersampled_text, undersampled_numeric], undersampled_labels, epochs=5, validation_data=([val_text, val_numeric], val_labels))"
      ],
      "metadata": {
        "id": "gCMpqdv1DSBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X6hTf4wp76x"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, label='Training loss')\n",
        "plt.plot(epochs, val_loss, label='Validation loss')\n",
        "plt.title('Training and validation loss on Undersampled Training Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65fbyVZcp9tc"
      },
      "source": [
        "auc = history.history['auc']\n",
        "val_auc = history.history['val_auc']\n",
        "epochs = range(1, len(auc) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, auc, label='Training AUC')\n",
        "plt.plot(epochs, val_auc, label='Validation AUC')\n",
        "plt.title('Training and validation AUC on Undersampled Training Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75C6luiKqEdP"
      },
      "source": [
        "fp = history.history['fp']\n",
        "val_fp = history.history['val_fp']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, fp, label='Training False Positives')\n",
        "plt.plot(epochs, val_fp, label='Validation False Positives')\n",
        "plt.title('Training and validation False Positives on Undersampled Training Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtuzddQKa9aZ"
      },
      "source": [
        "fn = history.history['fn']\n",
        "val_fn = history.history['val_fn']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, label='Training False Negatives')\n",
        "plt.plot(epochs, val_loss, label='Validation False Negatives')\n",
        "plt.title('Training and validation False Negatives on Undersampled Training Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLVqcqeiASQv"
      },
      "source": [
        "The mean undersampled validation AUC was only barely lower than the mean validation AUC for the entire dataset:\n",
        "* All Training Data: 0.5054\n",
        "* Undersampled Training Data: 0.5010\n",
        "\n",
        "That being said, an AUC of roughly 50% is not good at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5peBr2FC4pzA"
      },
      "source": [
        "#### Retraining the entire model for the test set with text + numeric variables\n",
        "Note: I ran this section initially but due to the low AUC values during the training phase, I know that the text-only data significantly improved the AUC so I will not rerun this section. All code was left but it has been commented out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeUp6By-OgRt"
      },
      "source": [
        "# total_train_numeric=np.concatenate((train_numeric,val_numeric))\n",
        "# total_train_text=np.concatenate((train_text,val_text))\n",
        "# total_train_labels= np.concatenate((train_labels, val_labels))\n",
        "\n",
        "# model_rnn2 = build_rnn_model()\n",
        "# model_rnn2.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "#                     loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "#                     metrics=metrics_list)\n",
        "# print(model_rnn2.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBHqCB_GnAJz"
      },
      "source": [
        "# keras.utils.plot_model(model_rnn2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAmqT0DVOgPY"
      },
      "source": [
        "# history2 = model_rnn2.fit([total_train_text, total_train_numeric], total_train_labels, epochs=5, validation_data=([test_text, test_numeric], test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yk33DOM_sU8"
      },
      "source": [
        "# history2.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee5t5DHj75Yk"
      },
      "source": [
        "# combo_auc = history2.history['auc']\n",
        "# test_auc = history2.history['val_auc']\n",
        "# print(f\"Train+Val Average Mean: {mean(combo_auc)}; Test Average Mean: {mean(test_auc)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9QRVIQE2bi8"
      },
      "source": [
        "# loss2 = history2.history['loss']\n",
        "# val_loss2 = history2.history['val_loss']\n",
        "# epochs2 = range(1, len(loss2) + 1)\n",
        "# plt.figure()\n",
        "# plt.plot(epochs2, loss2, label='Training loss')\n",
        "# plt.plot(epochs2, val_loss2, label='Validation loss')\n",
        "# plt.title('Train+Validation and Testing loss (Text Only Data)')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOL7XPab2baA"
      },
      "source": [
        "# auc2 = history2.history['auc']\n",
        "# val_auc2 = history2.history['val_auc']\n",
        "# epochs2 = range(1, len(auc2) + 1)\n",
        "# plt.figure()\n",
        "# plt.plot(epochs2, auc2, label='Training AUC')\n",
        "# plt.plot(epochs2, val_auc2, label='Validation AUC')\n",
        "# plt.title('Train+Validation and Testing AUC (Text Only Data)')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gScmrfcE2zzW"
      },
      "source": [
        "### Text-Only Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEOIkYJcFbY2"
      },
      "source": [
        "#### Create Text-Only Data \\--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeX3J7UA8tNx"
      },
      "source": [
        "df_text_only = df_preproc_done.copy()\n",
        "df_text_only.drop(columns=['work_remote', 'has_company_logo', 'has_questions', 'salary_low', 'salary_high'],inplace=True)\n",
        "df_text_only.head(3)\n",
        "df_text = df_text_only.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNvn7x9KK-fg"
      },
      "source": [
        "df_text.title = df_text['title'] + ' ' + df_text['location']  + ' ' + df_text['description']\n",
        "df_text.department = df_text['department'] + ' ' + df_text['employment_type'] + ' ' + df_text['required_experience'] + ' ' + df_text['industry'] + ' ' + df_text['function'] + ' ' + df_text['required_education']\n",
        "df_text.drop(columns=['location', 'description', 'employment_type', 'required_experience', 'industry', 'function', 'required_education'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmkEhDdhKI1S"
      },
      "source": [
        "#replace 'No Data' with empty space ??\n",
        "df_text.rename(columns={'title': 'title_loc_description', 'department': 'cat_vars'}, inplace=True)\n",
        "df_text.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EdPORAD90Y3"
      },
      "source": [
        "#RECREATE TRAIN/VAL/TEST SPLIT WITH TEXT-ONLY DATA\n",
        "\n",
        "print('dataframe shape:',df_text.shape)\n",
        "train2, test_t = train_test_split(df_text, train_size=0.8, random_state=1, shuffle=True, stratify=df_text.fraudulent.values)\n",
        "# print(\"train shape:\",train.shape)\n",
        "\n",
        "train_t, val_t= train_test_split(train2, train_size = 0.8, random_state=1, shuffle = True, stratify=train2.fraudulent.values)\n",
        "print(\"train_t shape:\",train_t.shape)\n",
        "print('val_t shape:',val_t.shape)\n",
        "print('test_t shape:',test_t.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsx2ozK5NkGK"
      },
      "source": [
        "text_combos = df_text.columns[:-2] #removes full_text and fraudulent columns\n",
        "text_combos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jtkFbCcGCKc"
      },
      "source": [
        "#title_loc_description,\tcat_vars,\tcompany_profile,\trequirements,\tbenefits\n",
        "train_text1 = train_t['title_loc_description'].to_numpy()\n",
        "train_text2 = train_t['cat_vars'].to_numpy()\n",
        "train_text3 = train_t['company_profile'].to_numpy()\n",
        "train_text4 = train_t['requirements'].to_numpy()\n",
        "train_text5 = train_t['benefits'].to_numpy()\n",
        "train_labels = train_t['fraudulent'].to_numpy()\n",
        "\n",
        "val_text1 = val_t['title_loc_description'].to_numpy()\n",
        "val_text2 = val_t['cat_vars'].to_numpy()\n",
        "val_text3 = val_t['company_profile'].to_numpy()\n",
        "val_text4 = val_t['requirements'].to_numpy()\n",
        "val_text5 = val_t['benefits'].to_numpy()\n",
        "val_labels = val_t['fraudulent'].to_numpy()\n",
        "\n",
        "test_text1 = test_t['title_loc_description'].to_numpy()\n",
        "test_text2 = test_t['cat_vars'].to_numpy()\n",
        "test_text3 = test_t['company_profile'].to_numpy()\n",
        "test_text4 = test_t['requirements'].to_numpy()\n",
        "test_text5 = test_t['benefits'].to_numpy()\n",
        "test_labels = test_t['fraudulent'].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywbiopR38mCa"
      },
      "source": [
        "#### Building Text-Only RNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBYt-OXzNy8G"
      },
      "source": [
        "text = df_text['title_loc_description'].values\n",
        "vectorize1=vectorize(text)\n",
        "\n",
        "text = df_text['cat_vars'].values\n",
        "vectorize2=vectorize(text)\n",
        "\n",
        "text = df_text['company_profile'].values\n",
        "vectorize3=vectorize(text)\n",
        "\n",
        "text = df_text['requirements'].values\n",
        "vectorize4=vectorize(text)\n",
        "\n",
        "text = df_text['benefits'].values\n",
        "vectorize5=vectorize(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(text):\n",
        "  vectorize_layer = TextVectorization(\n",
        "      output_mode='int',\n",
        "      max_tokens=128)\n",
        "  #builds vocabulary\n",
        "  vectorize_layer.adapt(np.asarray(text))\n",
        "  return vectorize_layer #return the layer instance, not the result of calling it\n",
        "\n",
        "\n",
        "#title_loc_description,\tcat_vars,\tcompany_profile,\trequirements,\tbenefits\n",
        "text_data = [df_text['title_loc_description'].values, df_text['cat_vars'].values, df_text['company_profile'].values, df_text['requirements'].values, df_text['benefits'].values]\n",
        "vectorize1=vectorize(text_data[0])\n",
        "vectorize2=vectorize(text_data[1])\n",
        "vectorize3=vectorize(text_data[2])\n",
        "vectorize4=vectorize(text_data[3])\n",
        "vectorize5=vectorize(text_data[4])\n",
        "\n",
        "vects = [vectorize1, vectorize2, vectorize3, vectorize4, vectorize5]\n",
        "for i, v in enumerate(vects):\n",
        "  #The adapt call should already be performed within the vectorize function\n",
        "  #v.adapt(text_data[i]) #Use the appropriate text data for each vectorizer\n",
        "  vocab = v.get_vocabulary() #v is now a TextVectorization layer\n",
        "  print(f\"Total Length of vocab: {len(vocab)}\\nTop 20 words in vocab: {vocab[:20]}\\nLast 20 words in vocab: {vocab[-20:]}\\n\")"
      ],
      "metadata": {
        "id": "1prSEcinKuyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check 301"
      ],
      "metadata": {
        "id": "8Vj5Go7-LE3n"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQwzNBNA8pj9"
      },
      "source": [
        "metrics_list = [\n",
        "        keras.metrics.FalsePositives(name='fp'),\n",
        "        keras.metrics.FalseNegatives(name='fn'),\n",
        "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "        keras.metrics.AUC(name='auc')\n",
        "  ]\n",
        "\n",
        "def build_text_rnn():\n",
        "  inputs1=tf.keras.Input(shape=(1,), dtype=tf.string, name='inputs1')\n",
        "  outputs1 = vectorize1(inputs1) #text_outputs.dtype = 'tf.int64'\n",
        "  a = layers.Embedding(len(vectorize1.get_vocabulary()), 128, input_length=128, mask_zero=True)(outputs1)\n",
        "  a=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(a)\n",
        "\n",
        "  inputs2=tf.keras.Input(shape=(1,), dtype=tf.string, name='inputs2')\n",
        "  outputs2 = vectorize2(inputs2) #text_outputs.dtype = 'tf.int64'\n",
        "  b = layers.Embedding(len(vectorize2.get_vocabulary()), 128, input_length=128, mask_zero=True)(outputs2)\n",
        "  b=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(b)\n",
        "\n",
        "  inputs3=tf.keras.Input(shape=(1,), dtype=tf.string, name='inputs3')\n",
        "  outputs3 = vectorize3(inputs3) #text_outputs.dtype = 'tf.int64'\n",
        "  c = layers.Embedding(len(vectorize3.get_vocabulary()), 128, input_length=128, mask_zero=True)(outputs3)\n",
        "  c=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(c)\n",
        "\n",
        "  inputs4=tf.keras.Input(shape=(1,), dtype=tf.string, name='inputs4')\n",
        "  outputs4 = vectorize4(inputs4) #text_outputs.dtype = 'tf.int64'\n",
        "  d = layers.Embedding(len(vectorize4.get_vocabulary()), 128, input_length=128, mask_zero=True)(outputs4)\n",
        "  d=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(d)\n",
        "\n",
        "  inputs5=tf.keras.Input(shape=(1,), dtype=tf.string, name='inputs5')\n",
        "  outputs5 = vectorize5(inputs5) #text_outputs.dtype = 'tf.int64'\n",
        "  e = layers.Embedding(len(vectorize5.get_vocabulary()), 128, input_length=128, mask_zero=True)(outputs5)\n",
        "  e=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(e)\n",
        "\n",
        "\n",
        "  inputs = keras.layers.concatenate([a,b,c,d,e])\n",
        "  x = layers.Dense(32, activation='relu', name='layer1')(inputs)\n",
        "  # x=layers.Dropout(0.2, name='dropout1')(x)\n",
        "  # x = layers.Dense(32, activation='relu', name='layer2')(x)\n",
        "  # x=layers.Dropout(0.2, name='dropout2')(x)\n",
        "  output=layers.Dense(1, activation='sigmoid', name='final_output')(x)\n",
        "\n",
        "  rnn_model = Model(inputs=[inputs1, inputs2, inputs3, inputs4, inputs5], outputs=[output])\n",
        "  return rnn_model\n",
        "\n",
        "model_rnn = build_text_rnn()\n",
        "print(model_rnn.summary())\n",
        "model_rnn.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "                    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                    metrics=metrics_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtQyJTH8xc45"
      },
      "source": [
        "keras.utils.plot_model(model_rnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr2huffGSbp7"
      },
      "source": [
        "#### Training the Text-Only Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08D3P1I4Sekh"
      },
      "source": [
        "history = model_rnn.fit([train_text1, train_text2,train_text3,train_text4,train_text5], train_labels, epochs=5, validation_data=([val_text1, val_text2,val_text3,val_text4,val_text5], val_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz6alHnb1Fou"
      },
      "source": [
        "history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRIvq3BC2N9f"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, label='Training loss')\n",
        "plt.plot(epochs, val_loss, label='Validation loss')\n",
        "plt.title('Training and validation loss (Text Only Data)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvEjmFcrg4Wy"
      },
      "source": [
        "auc = history.history['auc']\n",
        "val_auc = history.history['val_auc']\n",
        "epochs = range(1, len(auc) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, auc, label='Training AUC')\n",
        "plt.plot(epochs, val_auc, label='Validation AUC')\n",
        "plt.title('Training and validation AUC (Text Only Data)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DtMs6vxg4W3"
      },
      "source": [
        "fp = history.history['fp']\n",
        "val_fp = history.history['val_fp']\n",
        "epochs = range(1, len(fp) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, fp, label='Training False Positives')\n",
        "plt.plot(epochs, val_fp, label='Validation False Positives')\n",
        "plt.title('Training and validation False Positives (Text Only Data)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02lTdD6Pg4W4"
      },
      "source": [
        "fn = history.history['fn']\n",
        "val_fn = history.history['val_fn']\n",
        "epochs = range(1, len(fn) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, fn, label='Training False Negatives')\n",
        "plt.plot(epochs,  val_fn, label='Validation False Negatives')\n",
        "plt.title('Training and validation False Negatives (Text Only Data)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR_CGGX3-N_F"
      },
      "source": [
        "#### Applying RNN on Test Set (Text-Only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YnhNIla6ABj"
      },
      "source": [
        "all_text1=np.concatenate((train_text1, val_text1))\n",
        "all_text2=np.concatenate((train_text2, val_text2))\n",
        "all_text3=np.concatenate((train_text3, val_text3))\n",
        "all_text4=np.concatenate((train_text4, val_text4))\n",
        "all_text5=np.concatenate((train_text5, val_text5))\n",
        "\n",
        "all_labels= np.concatenate((train_labels, val_labels))\n",
        "\n",
        "model_rnn2 = build_text_rnn()\n",
        "model_rnn2.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "                    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                    metrics=metrics_list)\n",
        "print(model_rnn2.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne9SNMD05_4R"
      },
      "source": [
        "history2 = model_rnn2.fit([all_text1, all_text2, all_text3, all_text4, all_text5], all_labels, epochs=5, validation_data=([test_text1, test_text2, test_text3, test_text4, test_text5], test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppGGuWQ05_0_"
      },
      "source": [
        "history2.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qQTayrC11ET"
      },
      "source": [
        "loss2 = history2.history['loss']\n",
        "val_loss2 = history2.history['val_loss']\n",
        "epochs2 = range(1, len(loss2) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs2, loss2, label='Training loss')\n",
        "plt.plot(epochs2, val_loss2, label='Validation loss')\n",
        "plt.title('Train+Validation and Testing loss (Text Only Data)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9I3kLOOaGiu"
      },
      "source": [
        "auc2 = history2.history['auc']\n",
        "val_auc2 = history2.history['val_auc']\n",
        "epochs2 = range(1, len(auc2) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs2, auc2, label='Training AUC')\n",
        "plt.plot(epochs2, val_auc2, label='Validation AUC')\n",
        "plt.title('Train+Validation and Testing AUC (Text Only Data)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXqVNki0CUx9"
      },
      "source": [
        "fp2 = history.history['fp']\n",
        "val_fp2 = history.history['val_fp']\n",
        "epochs = range(1, len(fp2) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, fp2, label='Training False Positives')\n",
        "plt.plot(epochs, val_fp2, label='Validation False Positives')\n",
        "plt.title('Training+Validation / Testing False Positives (Text Only Data)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcr2g9K1CXKe"
      },
      "source": [
        "fn2 = history.history['fn']\n",
        "val_fn2 = history.history['val_fn']\n",
        "epochs = range(1, len(fn2) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, fn2, label='Training False Negatives')\n",
        "plt.plot(epochs,  val_fn2, label='Validation False Negatives')\n",
        "plt.title('Training+Validation / Testing False Negatives (Text Only Data)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epR0U5xvh1R0"
      },
      "source": [
        "## Model with BERT  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdSQYwBNN9Er"
      },
      "source": [
        "### Installing Dependencies & Exploring BERT \\--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzM0KQCzbNkd"
      },
      "source": [
        "#BERT dependencies\n",
        "!pip install -q tensorflow-text\n",
        "!pip install -q tf-models-official"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa_qHT5AbNd4"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text # A dependency of the preprocessing model\n",
        "from official.nlp import optimization  # to create AdamW optmizer\n",
        "\n",
        "tf.get_logger().setLevel('ERROR') #filters out all messages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJU2QYkKCHk4"
      },
      "source": [
        "# classic_bert_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4' #bert_en_uncased_L-12_H-768_A-12\n",
        "small_bert_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/2' ##considered small bert\n",
        "# small_bert2_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/2' #considered tiny bert\n",
        "\n",
        "preprocess_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\" #same preprocess layer for all bert models that I will use\n",
        "\n",
        "# print('Classic BERT model selected            :', classic_bert_encoder)\n",
        "print('Small BERT model selected           :', small_bert_encoder)\n",
        "# print('Small BERT model #2 selected           :', small_bert2_encoder)\n",
        "print('Preprocessing model auto-selected      :', preprocess_bert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w3KKAQNIPmc"
      },
      "source": [
        "text_test1 = [train_text1[0][:100]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afMSzFfWbNbA"
      },
      "source": [
        "bert_preprocess_model = hub.KerasLayer(preprocess_bert)\n",
        "text_preprocessed = bert_preprocess_model(text_test1)\n",
        "\n",
        "print(f'Keys            : {list(text_preprocessed.keys())}')\n",
        "print(f'Word Ids Shape  : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids        : {text_preprocessed[\"input_word_ids\"][0, :30]}')\n",
        "print(f'Input Mask Shape: {text_preprocessed[\"input_mask\"].shape}')\n",
        "print(f'Input Mask      : {text_preprocessed[\"input_mask\"][0, :30]}')\n",
        "print(f'Type Ids Shape  : {text_preprocessed[\"input_type_ids\"].shape}')\n",
        "print(f'Type Ids        : {text_preprocessed[\"input_type_ids\"][0, :30]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Tm1ixzZy_v"
      },
      "source": [
        "bert_encoders = [small_bert_encoder] #small_bert2_encoder, classic_bert_encoder\n",
        "\n",
        "for be in bert_encoders:\n",
        "  bert_model = hub.KerasLayer(be) #The BERT models return a map with 3 important keys: pooled_output, sequence_output, encoder_outputs\n",
        "  bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "  print(f'Loaded BERT: {be}')\n",
        "  print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}') #The shape is [batch_size, H]. You can think of this as an embedding for the entire movie review.\n",
        "  print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}') #For the fine-tuning you are going to use the pooled_output array\n",
        "  print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "  print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOWHtD2NF5WE"
      },
      "source": [
        "### Defining the Model \\--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhCxCi3R3EgI"
      },
      "source": [
        "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
        "  \"\"\"Returns Model mapping string features to BERT inputs.\n",
        "\n",
        "  Args:\n",
        "    sentence_features: a list with the names of string-valued features.\n",
        "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model that can be called on a list or dict of string Tensors\n",
        "    (with the order or names, resp., given by sentence_features) and\n",
        "    returns a dict of tensors for input to BERT.\n",
        "  \"\"\"\n",
        "\n",
        "  input_segments = [tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft) for ft in sentence_features]\n",
        "\n",
        "  # Tokenize the text to word pieces.\n",
        "  bert_preprocess = hub.load(preprocess_bert)\n",
        "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
        "  segments = [tokenizer(s) for s in input_segments]\n",
        "\n",
        "  # Optional: Trim segments in a smart way to fit seq_length.\n",
        "  # Simple cases (like this example) can skip this step and let\n",
        "  # the next step apply a default truncation to approximately equal lengths.\n",
        "  truncated_segments = segments\n",
        "\n",
        "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
        "  # are model-dependent, so this gets loaded from the SavedModel.\n",
        "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
        "                          arguments=dict(seq_length=seq_length),\n",
        "                          name='packer')\n",
        "  model_inputs = packer(truncated_segments)\n",
        "  return tf.keras.Model(input_segments, model_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IaOKpogP6lR"
      },
      "source": [
        "#PREPROCESSING FOR BERT - TRAIN DATA\n",
        "preproc_model1 = make_bert_preprocess_model(['title_loc_description'])\n",
        "text = [train_text1]\n",
        "preproc1 = preproc_model1(text)\n",
        "\n",
        "preproc_model2 = make_bert_preprocess_model(['cat_vars'])\n",
        "text = [train_text2]\n",
        "preproc2 = preproc_model2(text)\n",
        "\n",
        "preproc_model3 = make_bert_preprocess_model(['company_profile'])\n",
        "text = [train_text3]\n",
        "preproc3 = preproc_model3(text)\n",
        "\n",
        "preproc_model4 = make_bert_preprocess_model(['requirements'])\n",
        "text = [train_text4]\n",
        "preproc4 = preproc_model4(text)\n",
        "\n",
        "preproc_model5 = make_bert_preprocess_model(['benefits'])\n",
        "text = [train_text5]\n",
        "preproc5 = preproc_model5(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sUdq3cmduYg"
      },
      "source": [
        "print('preproc1 - training')\n",
        "print('Keys           : ', list(preproc1.keys()))\n",
        "print('Shape Word Ids : ', preproc1['input_word_ids'].shape)\n",
        "print('Word Ids       : ', preproc1['input_word_ids'][0, :20])\n",
        "print('Shape Mask     : ', preproc1['input_mask'].shape)\n",
        "print('Input Mask     : ', preproc1['input_mask'][0, :20])\n",
        "print('Shape Type Ids : ', preproc1['input_type_ids'].shape)\n",
        "print('Type Ids       : ', preproc1['input_type_ids'][0, :20])\n",
        "\n",
        "# print()\n",
        "# print('preproc2 - training')\n",
        "# print('Keys           : ', list(preproc2.keys()))\n",
        "# print('Shape Word Ids : ', preproc2['input_word_ids'].shape)\n",
        "# print('Word Ids       : ', preproc2['input_word_ids'][0, :16])\n",
        "# print('Shape Mask     : ', preproc2['input_mask'].shape)\n",
        "# print('Input Mask     : ', preproc2['input_mask'][0, :16])\n",
        "# print('Shape Type Ids : ', preproc2['input_type_ids'].shape)\n",
        "# print('Type Ids       : ', preproc2['input_type_ids'][0, :16])\n",
        "\n",
        "# print()\n",
        "# print('preproc3 - training')\n",
        "# print('Keys           : ', list(preproc3.keys()))\n",
        "# print('Shape Word Ids : ', preproc3['input_word_ids'].shape)\n",
        "# print('Word Ids       : ', preproc3['input_word_ids'][0, :16])\n",
        "# print('Shape Mask     : ', preproc3['input_mask'].shape)\n",
        "# print('Input Mask     : ', preproc3['input_mask'][0, :16])\n",
        "# print('Shape Type Ids : ', preproc3['input_type_ids'].shape)\n",
        "# print('Type Ids       : ', preproc3['input_type_ids'][0, :16])\n",
        "\n",
        "# print()\n",
        "# print('preproc4 - training')\n",
        "# print('Keys           : ', list(preproc4.keys()))\n",
        "# print('Shape Word Ids : ', preproc4['input_word_ids'].shape)\n",
        "# print('Word Ids       : ', preproc4['input_word_ids'][0, :16])\n",
        "# print('Shape Mask     : ', preproc4['input_mask'].shape)\n",
        "# print('Input Mask     : ', preproc4['input_mask'][0, :16])\n",
        "# print('Shape Type Ids : ', preproc4['input_type_ids'].shape)\n",
        "# print('Type Ids       : ', preproc4['input_type_ids'][0, :16])\n",
        "\n",
        "# print()\n",
        "# print('preproc5 - training')\n",
        "# print('Keys           : ', list(preproc5.keys()))\n",
        "# print('Shape Word Ids : ', preproc5['input_word_ids'].shape)\n",
        "# print('Word Ids       : ', preproc5['input_word_ids'][0, :16])\n",
        "# print('Shape Mask     : ', preproc5['input_mask'].shape)\n",
        "# print('Input Mask     : ', preproc5['input_mask'][0, :16])\n",
        "# print('Shape Type Ids : ', preproc5['input_type_ids'].shape)\n",
        "# print('Type Ids       : ', preproc5['input_type_ids'][0, :16])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlSA8zgFdYZx"
      },
      "source": [
        "tf.keras.utils.plot_model(preproc_model1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzWXEbUYeS01"
      },
      "source": [
        "tf.keras.utils.plot_model(preproc_model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe-AT5GZeSjb"
      },
      "source": [
        "tf.keras.utils.plot_model(preproc_model3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7IkcteUeSbk"
      },
      "source": [
        "tf.keras.utils.plot_model(preproc_model4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SFjd5nKeSSC"
      },
      "source": [
        "tf.keras.utils.plot_model(preproc_model5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRDqOWOOoGh6"
      },
      "source": [
        "#PREPROCESSING FOR BERT - VAL DATA\n",
        "val_texts=[val_text1, val_text2, val_text3, val_text4, val_text5]\n",
        "val_preproc1 = preproc_model1([val_texts[0]]) #title_loc_description\n",
        "val_preproc2 = preproc_model2([val_texts[1]]) #cat_vars\n",
        "val_preproc3 = preproc_model3([val_texts[2]]) #company_profile\n",
        "val_preproc4 = preproc_model4([val_texts[3]]) #requirements\n",
        "val_preproc5 = preproc_model5([val_texts[4]]) #benefits\n",
        "\n",
        "#PREPROCESSING FOR BERT - TEST DATA\n",
        "test_texts=[test_text1, test_text2, test_text3, test_text4, test_text5]\n",
        "test_preproc1 = preproc_model1([test_texts[0]]) #title_loc_description\n",
        "test_preproc2 = preproc_model2([test_texts[1]]) #cat_vars\n",
        "test_preproc3 = preproc_model3([test_texts[2]]) #company_profile\n",
        "test_preproc4 = preproc_model4([test_texts[3]]) #requirements\n",
        "test_preproc5 = preproc_model5([test_texts[4]]) #benefits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCyaZvCmiTB-"
      },
      "source": [
        "def build_bert_model(encoder):\n",
        "  inputs1 = dict(\n",
        "        input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_word_ids1'),\n",
        "        input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_mask1'),\n",
        "        input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_type_ids1')\n",
        "    )\n",
        "  encoder1 = hub.KerasLayer(encoder, trainable=True, name='BERT_encoder1')\n",
        "  net1=encoder1(inputs1)['pooled_output']\n",
        "\n",
        "  inputs2 = dict(\n",
        "        input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_word_ids2'),\n",
        "        input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_mask2'),\n",
        "        input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_type_ids2')\n",
        "    )\n",
        "  encoder2 = hub.KerasLayer(encoder, trainable=True, name='BERT_encoder2')\n",
        "  net2=encoder2(inputs2)['pooled_output']\n",
        "\n",
        "  inputs3 = dict(\n",
        "        input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_word_ids3'),\n",
        "        input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_mask3'),\n",
        "        input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_type_ids3')\n",
        "    )\n",
        "  encoder3 = hub.KerasLayer(encoder, trainable=True, name='BERT_encoder3')\n",
        "  net3=encoder3(inputs3)['pooled_output']\n",
        "\n",
        "  inputs4 = dict(\n",
        "        input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_word_ids4'),\n",
        "        input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_mask4'),\n",
        "        input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_type_ids4')\n",
        "    )\n",
        "  encoder4 = hub.KerasLayer(encoder, trainable=True, name='BERT_encoder4')\n",
        "  net4=encoder4(inputs4)['pooled_output']\n",
        "\n",
        "  inputs5 = dict(\n",
        "        input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_word_ids5'),\n",
        "        input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_mask5'),\n",
        "        input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_type_ids5')\n",
        "    )\n",
        "  encoder5 = hub.KerasLayer(encoder, trainable=True, name='BERT_encoder5')\n",
        "  net5=encoder5(inputs5)['pooled_output']\n",
        "\n",
        "  inputs = tf.keras.layers.concatenate([net1, net2, net3, net4, net5])\n",
        "  x = tf.keras.layers.Dropout(0.1)(inputs)\n",
        "  output = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(x)\n",
        "  return tf.keras.Model(inputs=[inputs1, inputs2, inputs3, inputs4, inputs5], outputs=[output])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j25wUkvn0ftR"
      },
      "source": [
        "small_bert_model = build_bert_model(small_bert_encoder)\n",
        "print(small_bert_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eLnVODq1TYf"
      },
      "source": [
        "tf.keras.utils.plot_model(small_bert_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6fi7d_kM0Lq"
      },
      "source": [
        "### Fine-Tuning BERT Model \\--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l2Ab_hx17A0"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "metrics_list = [\n",
        "        keras.metrics.FalsePositives(name='fp'),\n",
        "        keras.metrics.FalseNegatives(name='fn'),\n",
        "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "        keras.metrics.AUC(name='auc')\n",
        "  ]\n",
        "\n",
        "metrics = metrics_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqokz2Xs169s"
      },
      "source": [
        "epochs = 1\n",
        "steps_per_epoch = len(train_t)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2Oe19PkF435"
      },
      "source": [
        "print(steps_per_epoch)\n",
        "print(num_train_steps)\n",
        "print(num_warmup_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA218bkASPik"
      },
      "source": [
        "### Small BERT model (small_bert/bert_en_uncased_L-4_H-512_A-4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyLfB4e0166s"
      },
      "source": [
        "small_bert_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmMxYUvi1627"
      },
      "source": [
        "#training model with small bert\n",
        "history = small_bert_model.fit([preproc1, preproc2, preproc3, preproc4, preproc5], train_labels, epochs=1, validation_data=([val_preproc1, val_preproc2, val_preproc3, val_preproc4, val_preproc5], val_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB6wegE9gSDJ"
      },
      "source": [
        "history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbxK1Jy1Yx7e"
      },
      "source": [
        "### Classic BERT model (bert_en_uncased_L-12_H-768_A-12) --> CANNOT RUN ON COLAB CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLXfv1fcyRc0"
      },
      "source": [
        "# bert_model = build_bert_model(classic_bert_encoder)\n",
        "# print(bert_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzL9kQvqnCCo"
      },
      "source": [
        "# bert_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK7MkS3lhHHe"
      },
      "source": [
        "# CANNOT RUN -- BERT MODEL IS TOO LARGE FOR COLAB\n",
        "# history = bert_model.fit([preproc1, preproc2, preproc3, preproc4, preproc5], train_labels, epochs=1, validation_data=([val_preproc1, val_preproc2, val_preproc3, val_preproc4, val_preproc5], val_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmWIOsDMhG9A"
      },
      "source": [
        "# history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMtuB80AfTf-"
      },
      "source": [
        "### Applying the Small-BERT model to the testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y--t2Z342A9A"
      },
      "source": [
        "small_bert_model.evaluate([test_preproc1, test_preproc2, test_preproc3, test_preproc4, test_preproc5], test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FGLhfyMH2jPj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}